{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "# In this script, we will focus on generating an image\n",
    "# that attempts to match the content of one input image\n",
    "# and the style of another input image.\n",
    "#\n",
    "# We accomplish this by balancing the content loss\n",
    "# and style loss simultaneously.\n",
    "\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from skimage.transform import resize\n",
    "\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import cv2\n",
    "import time,random\n",
    "\n",
    "from style_transfer1 import VGG16_AvgPool, VGG16_AvgPool_CutOff, unpreprocess, scale_img\n",
    "from style_transfer2 import gram_matrix, style_loss, minimize\n",
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f5984e36cfcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0munq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000.0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0moutpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mcontent_level\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# load the content image\n",
    "def load_img_and_preprocess(path, shape=None):\n",
    "  img = image.load_img(path, target_size=shape)\n",
    "\n",
    "  # convert image to array and preprocess for vgg\n",
    "  x = image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  x = preprocess_input(x)\n",
    "\n",
    "  return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    projects = '/home/mbastola/datascience/projects/'\n",
    "    path = projects+'neural_style_transfer/results/'\n",
    "    unq = int( time.time() * 1000.0 )\n",
    "\n",
    "    outpath = path + str(unq)+sys.argv[3]\n",
    "    content_level = int(sys.argv[4]);\n",
    "\n",
    "    content_img = load_img_and_preprocess(path+'inputs/'+sys.argv[1])\n",
    "\n",
    "    # resize the style image\n",
    "    # since we don't care too much about warping it\n",
    "    h, w = content_img.shape[1:3]\n",
    "\n",
    "    style_img = load_img_and_preprocess(path+'inputs/'+sys.argv[2], (h,w));\n",
    "    # we'll use this throughout the rest of the script\n",
    "    batch_shape = content_img.shape\n",
    "    shape = content_img.shape[1:]\n",
    "\n",
    "\n",
    "    # we want to make only 1 VGG here\n",
    "    # as you'll see later, the final model needs\n",
    "    # to have a common input\n",
    "    vgg = VGG16_AvgPool(shape)\n",
    "\n",
    "    # create the content model\n",
    "    # we only want 1 output\n",
    "    # remember you can call vgg.summary() to see a list of layers\n",
    "    # 1,2,4,5,7-9,11-13,15-17\n",
    "    print(vgg.layers)\n",
    "    exit\n",
    "\n",
    "    content_model = Model(vgg.input, vgg.layers[content_level].get_output_at(0))\n",
    "    content_target = K.variable(content_model.predict(content_img))\n",
    "\n",
    "\n",
    "    # create the style model\n",
    "    # we want multiple outputs\n",
    "    # we will take the same approach as in style_transfer2.py\n",
    "    symbolic_conv_outputs = [\n",
    "        layer.get_output_at(1) for layer in vgg.layers \\\n",
    "        if layer.name.endswith('conv1')\n",
    "    ]\n",
    "\n",
    "    # make a big model that outputs multiple layers' outputs\n",
    "    style_model = Model(vgg.input, symbolic_conv_outputs)\n",
    "\n",
    "    # calculate the targets that are output at each layer\n",
    "    style_layers_outputs = [K.variable(y) for y in style_model.predict(style_img)]\n",
    "\n",
    "    # we will assume the weight of the content loss is 1\n",
    "    # and only weight the style losses\n",
    "    #style_weights = [0.2,0.4,0.3,0.5,0.2]\n",
    "    #style_weights = [0.2,0.6,0.1,0.2,0.1]\n",
    "    #style_weights = [0.4,0.6,0.1,0.8,0.6]\n",
    "    style_weights = [0.2,0.2,0.2,0.2,0.2]\n",
    "\n",
    "    # create the total loss which is the sum of content + style loss\n",
    "    loss = K.mean(K.square(content_model.output - content_target))\n",
    "\n",
    "    for w, symbolic, actual in zip(style_weights, symbolic_conv_outputs, style_layers_outputs):\n",
    "        # gram_matrix() expects a (H, W, C) as input\n",
    "        loss += w * style_loss(symbolic[0], actual[0])\n",
    "\n",
    "\n",
    "    #once again, create the gradients and loss + grads function\n",
    "    # note: it doesn't matter which model's input you use\n",
    "    # they are both pointing to the same keras Input layer in memory\n",
    "    grads = K.gradients(loss, vgg.input)\n",
    "\n",
    "    # just like theano.function\n",
    "    get_loss_and_grads = K.function(\n",
    "        inputs=[vgg.input],\n",
    "        outputs=[loss] + grads\n",
    "    )\n",
    "\n",
    "\n",
    "    def get_loss_and_grads_wrapper(x_vec):\n",
    "        l, g = get_loss_and_grads([x_vec.reshape(*batch_shape)])\n",
    "        return l.astype(np.float64), g.flatten().astype(np.float64)\n",
    "\n",
    "    final_img = minimize(get_loss_and_grads_wrapper, 10, batch_shape)\n",
    "\n",
    "    scaled = scale_img(final_img)\n",
    "    plt.imshow(scaled)\n",
    "    plt.show()\n",
    "\n",
    "    #plt.imshow(scale_img(final_img))\n",
    "    #plt.show()\n",
    "\n",
    "    b,g,r = cv2.split(255*scaled);\n",
    "    out = cv2.merge((r,g,b))\n",
    "    cv2.imwrite(outpath, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (674,622,3)\n",
    "vgg = VGG16_AvgPool(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x7f928fef3780>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f928fef3cf8>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f92904b2860>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f92904f32b0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f928fee8e80>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f92904b2550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f928fdcc6d8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f9290f980b8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f9290f8ef98>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f92904b25f8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f9290f8b978>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f928ff12f60>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f928fd18860>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f9290569d30>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f9290470d68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f928fe7a160>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f928fe89860>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7f9290426320>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
